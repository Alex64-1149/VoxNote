{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex64-1149/VoxNote/blob/IA-VoxNote/VoxNote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptation des données"
      ],
      "metadata": {
        "id": "zeFHx1nkShwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Alphabet dans : https://github.com/dallal9/French-NLP"
      ],
      "metadata": {
        "id": "gO8hYysRS1Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialisation des caractères possibles\n",
        "alphabet = [' ','a', 'à', 'A', 'Á', 'À', 'Â', 'Ä', 'Ã', 'Å', 'r', 'd', 'v', 'k', 's', 'b', 'i', 'e', 'n', 't', 'â', 'm', 'z', 'o', 'é', 'è', 'q', 'u', 'î', '-', 'j', 'g', 'l', 'x', 'y', 'c', 'ê', 'h', 'f', 'p', 'B', 'S', 'û', 'ç', 'ô', 'X', 'ï', 'D', 'N', 'Æ', 'G', 'T', 'C', 'ë', 'K', 'L', 'w', 'ö', 'P', 'I', 'R', \"'\", 'Z', 'E', 'H', 'Ç', 'O', 'M', 'É', 'U', 'V', 'Ð', 'ü', 'È', 'Ê', 'Ë', 'F', 'Q', '3', 'W', 'Í', 'Ì', 'Î', 'Ï', 'J', 'Y', '/', 'Ñ', 'Ó', 'Ò', 'Ô', 'Ö', 'Õ', 'Ø', 'ù', 'þ', 'Ú', 'Ù', 'Û', 'Ü', 'Ý']\n",
        "chiffres = ['1','2','3','4','5','6','7','8','9','0']\n",
        "caracteresPossible = alphabet + chiffres\n"
      ],
      "metadata": {
        "id": "jKqKD9Si5JK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#adaptation de https://github.com/musikalkemist/pytorchforaudio\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class ReconnaissanceVocaleDataset(Dataset) :\n",
        "\n",
        "  #Initialise les aurguments de la classe dataset:\n",
        "  # 1.fichier annotation(fichier txt qui contient tous les noms des fichiers audio (wav))\n",
        "  # 2.fichier audio ( contient tous les fichiers audio(wav))\n",
        "  def __init__(self,FICHIER_ANNOTE,FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur):\n",
        "    self.fichierAnnotations=pd.read_csv(FICHIER_ANNOTE)\n",
        "    self.fichierAudio=FICHIER_AUDIO\n",
        "    self.processeur = processeur\n",
        "    self.melSpectrogram = melSpectrogram.to(processeur) #s'assurer que tout se fasse au même endroit dans l'ordinateur : https://stackoverflow.com/questions/63061779/pytorch-when-do-i-need-to-use-todevice-on-a-model-or-tensor\n",
        "    self.SAMPLE_RATE_VOULU = SAMPLE_RATE\n",
        "    self.NOMBRE_ECHANTILLONS = NOMBRE_ECHANTILLONS\n",
        "\n",
        "  #retourne le nombre de fichiers dans notre dataset\n",
        "  def __len__(self):\n",
        "    return len(self.fichierAnnotations)\n",
        "\n",
        "  #retourne l'audio ainsi que son fichier texte associé\n",
        "  def __getitem__(self, index):\n",
        "    pathAudio = self.getAudioSamplePath(index)\n",
        "    texte = self.getAudioSampleText(index)\n",
        "    #signal informatique et sample rate de notre audio\n",
        "    signal, sr = torchaudio.load(pathAudio)\n",
        "\n",
        "    #mettre le signal au même endroit que sa transformation\n",
        "    signal = signal.to(processeur)\n",
        "\n",
        "    #normaliser l'audio\n",
        "    signal = self.resampleSiNecessaire(signal, sr)\n",
        "    signal = self.combinerSiNecessaire(signal)\n",
        "    #diminuer ou ajouter des échantillons \"vides\" si le nombre d'échantillons ne correspond à 22 050\n",
        "    signal = self.couperSiNecessaire(signal)\n",
        "    signal = self.paddingSiNecessaire(signal)\n",
        "\n",
        "    #transformer l'audio dans le spectogram de mel\n",
        "    signal = self.melSpectrogram(signal)\n",
        "    return signal, texte\n",
        "\n",
        "  #mettre tous les fichiers audios à la même fréquence d'échantillonage\n",
        "  def resampleSiNecessaire(self, signal, sr):\n",
        "    if sr != self.SAMPLE_RATE_VOULU :\n",
        "      resampler = torchaudio.transforms.Resample(sr, self.SAMPLE_RATE_VOULU)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  #s'assurer que l'audio ne contient qu'une entrée et sortie (que le son ne soit pas stéréo) pour le normaliser\n",
        "  def combinerSiNecessaire(self, signal):\n",
        "    if signal.shape[0] > 1 :\n",
        "      signal = torch.mean(signal, dim=0, keepdim= True)\n",
        "    return signal\n",
        "\n",
        "  #enlever les échantillons audios superflus\n",
        "  def couperSiNecessaire(self, signal) :\n",
        "    #le signal est composé de 2 dimensions : [nombre de source du signal(1 dans ce cas), longueur du signal – nombre d'échantillons (on veut 22 050)]\n",
        "    if signal.shape[1] > self.NOMBRE_ECHANTILLONS :\n",
        "      signal = signal[:, :self.NOMBRE_ECHANTILLONS] #utilité de [:, :] : la première dimension est prise au complet et la deuxième jusqu'à l'atteinte du nombre d'échantillon (expliquer à https://youtu.be/WyJvrzVNkOc?list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&t=478)\n",
        "    return signal\n",
        "\n",
        "  def paddingSiNecessaire(self, signal) :\n",
        "    #la longueur du signal = signal.shape[1] comme expliqué précédemment\n",
        "    if signal.shape[1] < self.NOMBRE_ECHANTILLONS :\n",
        "      paddingDuSignal = (0, self.NOMBRE_ECHANTILLONS - signal.shape[1]) #(0, nombre d'échantillons manquants)\n",
        "      torch.nn.functional.pad(signal, paddingDuSignal) #ajoute le padding au signal (https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  #retourne le chemin pour avoir le bon fichier audio à un certain index du FICHIER_ANNOTE\n",
        "  def getAudioSamplePath(self, index):\n",
        "    return self.fichierAudio[index]\n",
        "\n",
        "  #retourne le fichier texte associé à l'audio d'un certain index du FICHIER_AUDIO\n",
        "  def getAudioSampleText(self, index):\n",
        "    return self.fichierAnnotations[index]\n",
        "\n",
        "\n",
        "FICHIER_ANNOTE = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_text.list\"\n",
        "FICHIER_AUDIO = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_wavs.list\"\n",
        "\n",
        "\n",
        "#nombre d'échantillons par secondes dans notre audio\n",
        "SAMPLE_RATE = 22050\n",
        "NOMBRE_ECHANTILLONS = 22050\n",
        "\n",
        "if torch.cuda.is_available(): #détermine ce qui exécute le programme (gpu préférable pour AI audio)\n",
        "    processeur = \"cuda\"\n",
        "else:\n",
        "    processeur = \"cpu\"\n",
        "print(f\"Utiliation du processeur {processeur}\")\n",
        "\n",
        "#le Spectogram de Mel est une échelle logarithmique utilisée pour mieux représenter les différences qu'un humain entend dans un fichier audio ce qui aide à l'analyse sonore\n",
        "melSpectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    SAMPLE_RATE,\n",
        "    n_fft=512, #longueur physique du signal optimale pour la reconnaissance vocale selon : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    hop_length=512, #nombre d'échantillon audio adjacents analysés par la transformée de fourier : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    n_mels=64 #nombre de séparations d'une seule fréquence optimale pour la reconnaisance vocale selon:https://stackoverflow.com/questions/62623975/why-128-mel-bands-are-used-in-mel-spectrograms\n",
        ")\n",
        "\n",
        "\n",
        "rvd = ReconnaissanceVocaleDataset(FICHIER_ANNOTE, FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur)\n"
      ],
      "metadata": {
        "id": "a4QSC_m-8aL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133d9792-d620-43a4-ecde-74dc86ab2d41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utiliation du cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #nécessaire qu début de chaque session\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL25QMQ9YtRV",
        "outputId": "17f706ec-fed2-42dc-af49-0a18b295244f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#utilisé pour extraire le zip de google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#!unzip file.zip\n"
      ],
      "metadata": {
        "id": "iSiu0ku4h3lz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Réseau de neurone"
      ],
      "metadata": {
        "id": "ydEmnsTDS43i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraîner"
      ],
      "metadata": {
        "id": "QN81VZbWTGeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tester\n"
      ],
      "metadata": {
        "id": "-derVaPjTKNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inférence"
      ],
      "metadata": {
        "id": "72q5Kr3CTMyK"
      }
    }
  ]
}