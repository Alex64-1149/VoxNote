{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex64-1149/VoxNote/blob/IA-VoxNote/VoxNote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFHx1nkShwn"
      },
      "source": [
        "# Adaptation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "a4QSC_m-8aL-"
      },
      "outputs": [],
      "source": [
        "\n",
        "#adaptation de https://github.com/musikalkemist/pytorchforaudio\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import linecache\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class ReconnaissanceVocaleDataset(Dataset) :\n",
        "\n",
        "  #Initialise les aurguments de la classe dataset:\n",
        "  # 1.fichier annotation(fichier txt qui contient tous les noms des fichiers audio (wav))\n",
        "  # 2.fichier audio ( contient tous les fichiers audio(wav))\n",
        "  def __init__(self,FICHIER_ANNOTE,FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur, texte_transforme): # __[...]__ = classe nécessaire a un dataset Pytorch\n",
        "    self.fichierAnnotations=FICHIER_ANNOTE\n",
        "    self.fichierAudio=FICHIER_AUDIO\n",
        "    self.audioPathInitial = \"/content/drive/MyDrive/ColabNotebooks/SiwisFrenchSpeechSynthesisDatabase/wavs/\" #chemin pour se rendre au fichier pas compris dans la liste de noms de fichiers\n",
        "    self.textPathInitial = \"/content/drive/MyDrive/ColabNotebooks/SiwisFrenchSpeechSynthesisDatabase/text/\"  #\"                                                                            \"\n",
        "    self.processeur = processeur\n",
        "    self.melSpectrogram = melSpectrogram.to(processeur) #s'assurer que tout se fasse au même endroit dans l'ordinateur : https://stackoverflow.com/questions/63061779/pytorch-when-do-i-need-to-use-todevice-on-a-model-or-tensor\n",
        "    self.SAMPLE_RATE_VOULU = SAMPLE_RATE\n",
        "    self.NOMBRE_ECHANTILLONS = NOMBRE_ECHANTILLONS\n",
        "    self.texte_transforme = texte_transforme\n",
        "\n",
        "  #retourne le nombre de fichiers dans notre dataset\n",
        "  def __len__(self):\n",
        "    return len(self.fichierAnnotations)\n",
        "\n",
        "  #retourne l'audio ainsi que son fichier texte associé\n",
        "  def __getitem__(self, index):\n",
        "    pathAudio = self.getAudioSamplePath(index)\n",
        "    texte = self.getAudioSampleText(index)\n",
        "\n",
        "    #signal informatique et sample rate de notre audio\n",
        "    signal, sr = torchaudio.load(pathAudio)  #pour lire un fichier wav avec torchaudio, on obtient le format de l'onde audio ainsi que son sample rate : https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html\n",
        "\n",
        "    #mettre le signal au même endroit que sa transformation\n",
        "    signal = signal.to(self.processeur)\n",
        "    #normaliser l'audio\n",
        "    signal = self.resampleSiNecessaire(signal, sr)\n",
        "    signal = self.combinerSiNecessaire(signal)\n",
        "\n",
        "    #diminuer ou ajouter des échantillons \"vides\" si le nombre d'échantillons ne correspond à 22 050\n",
        "    signal = self.couperSiNecessaire(signal)\n",
        "    signal = self.paddingSiNecessaire(signal)\n",
        "\n",
        "    #transformer l'audio dans le spectogram de mel\n",
        "    signal = self.melSpectrogram(signal)\n",
        "\n",
        "    return signal, texte\n",
        "\n",
        "  #mettre tous les fichiers audios à la même fréquence d'échantillonage\n",
        "  def resampleSiNecessaire(self, signal, sr):\n",
        "\n",
        "    if sr != self.SAMPLE_RATE_VOULU :\n",
        "      resampler = torchaudio.transforms.Resample(sr, self.SAMPLE_RATE_VOULU).to(self.processeur)\n",
        "      signal = resampler(signal)\n",
        "\n",
        "    return signal\n",
        "\n",
        "  #s'assurer que l'audio ne contient qu'une entrée et sortie (que le son ne soit pas stéréo) pour le normaliser\n",
        "  def combinerSiNecessaire(self, signal):\n",
        "    if signal.shape[0] > 1 :\n",
        "      signal = torch.mean(signal, dim=0, keepdim= True)\n",
        "    return signal\n",
        "\n",
        "  #enlever les échantillons audios en surplus du nombre visé\n",
        "  def couperSiNecessaire(self, signal) :\n",
        "    #le signal est composé de 2 dimensions : [nombre de source du signal(1 dans ce cas), longueur du signal – nombre d'échantillons (on veut 22 050)]\n",
        "    if signal.shape[1] > self.NOMBRE_ECHANTILLONS :\n",
        "      signal = signal[:, :self.NOMBRE_ECHANTILLONS] #utilité de [:, :] : la première dimension est prise au complet et la deuxième jusqu'à l'atteinte du nombre d'échantillon (expliquer à https://youtu.be/WyJvrzVNkOc?list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&t=478)\n",
        "    return signal\n",
        "\n",
        "  def paddingSiNecessaire(self, signal) :\n",
        "    #la longueur du signal = signal.shape[1] comme expliqué précédemment\n",
        "    if signal.shape[1] < self.NOMBRE_ECHANTILLONS :\n",
        "      paddingDuSignal = (0, self.NOMBRE_ECHANTILLONS - signal.shape[1]) #(0, nombre d'échantillons manquants)\n",
        "      torch.nn.functional.pad(signal, paddingDuSignal) #ajoute le padding au signal (https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  #retourne le chemin pour avoir le bon fichier audio à un certain index du FICHIER_ANNOTE\n",
        "  def getAudioSamplePath(self, index):\n",
        "\n",
        "    index += 1 #la fonction considère la première ligne comme étant la ligne 1 (et non la ligne 0 comme habituellement)\n",
        "    audioPathFin = linecache.getline(self.fichierAudio, index)\n",
        "    audioPathFin = audioPathFin.strip() #pour enlever le \\n : https://www.geeksforgeeks.org/python-removing-newline-character-from-string/\n",
        "\n",
        "    return self.audioPathInitial + audioPathFin\n",
        "\n",
        "  #retourne le fichier texte associé à l'audio d'un certain index du FICHIER_AUDIO\n",
        "  def getAudioSampleText(self, index):\n",
        "\n",
        "    index += 1\n",
        "    textPathFin = linecache.getline(self.fichierAnnotations, index)\n",
        "    textPathFin = textPathFin.strip()\n",
        "\n",
        "    #encoding pour lire des textes en français trouvé sur : https://stackoverflow.com/questions/18649512/unicodedecodeerror-ascii-codec-cant-decode-byte-0xe2-in-position-13-ordinal\n",
        "    fichier = open(self.textPathInitial + textPathFin,\"r\",  encoding=\"utf-8\") #comment lire un fichier sur python : https://www.tutorialspoint.com/how-to-read-a-text-file-in-python\n",
        "    texte = fichier.read()\n",
        "    fichier.close()\n",
        "\n",
        "    return self.convertirStringEnTensor(texte)\n",
        "\n",
        "  def convertirStringEnTensor(self, texte): #concept pour convertir un string en tensor vient de : https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
        "    tensor = torch.zeros(500, 1, 54) #longueur maximum string * 1 * nombre caractères possibles\n",
        "    for ligne, lettre in enumerate(texte):\n",
        "      tensor[ligne][0][self.texte_transforme.text_to_int(lettre)] = 1 #avoir la valeur de la lettre en int dans le tableau = 1 et le reste = 0\n",
        "      if ligne == 499: #couper la string si ça dépasse 500 (comme dans couperSiNécessaire)\n",
        "        break\n",
        "    return tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL25QMQ9YtRV",
        "outputId": "6d2ee218-7218-43e4-927c-c53157d92f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive #nécessaire qu début de chaque session\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "iSiu0ku4h3lz"
      },
      "outputs": [],
      "source": [
        "#utilisé pour extraire le zip de google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#!unzip file.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydEmnsTDS43i"
      },
      "source": [
        "# Réseau de neurone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "zFqcIJnIc4ZK"
      },
      "outputs": [],
      "source": [
        "#connaissances nécessaires au RNN(LSTM) trouvées à l'adresse suivante : https://ketanhdoshi.github.io/Audio-ASR/\n",
        "#CNN initial vient de https://www.youtube.com/watch?v=SQ1iIKs190Q&list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&index=8\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "class BiRNNetwork(nn.Module) : #RNN de type bidirectional LSTM\n",
        "\n",
        "  def __init__(self, n_caracteres, rnn_dim, n_lstm_layers, dropout) :\n",
        "    super().__init__()\n",
        "    # 5 RNN blocks / flatten / linear /softmax\n",
        "    \"\"\"\n",
        "    1. Créer le bi-lstm (une sous catégorie des RNN) : https://medium.com/@anishnama20/understanding-bidirectional-lstm-for-sequential-data-processing-b83d6283befc\n",
        "    2. 'flatten' le résultat en diminuant le nombre de dimensions créées avec les blocs du RNN\n",
        "    3. transformer en équation linéaire les données fournies : https://docs.kanaries.net/topics/Python/nn-linear\n",
        "    4. normaliser les résultats à l'aide de softmax\n",
        "    \"\"\"\n",
        "    self.n_caracteres = n_caracteres\n",
        "    self.rnn_dim = rnn_dim\n",
        "    self.n_lstm_layers = n_lstm_layers\n",
        "    self.dropout = dropout\n",
        "    #self.batchSize = batchSize #nombre d'échantillon avant de changer les paramètres : https://datascience.stackexchange.com/questions/36651/relationship-between-batch-size-and-the-number-of-neurons-in-the-input-layer\n",
        "\n",
        "\n",
        "    \"\"\"self.conv1 = nn.Sequential(\n",
        "        nn.Dropout(0.1),#réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "        nn.Conv2d( #couche en 2 dimensions\n",
        "            in_channels=inChannels, #nombre de input initial (=1 lors de l'adaptation des données)\n",
        "            out_channels=outChannels, #nombre de filtre dans cette couche du réseau de neurone\n",
        "            kernel_size=3, #nombre de choses analysées en même temps : https://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean\n",
        "            stride = 1, #déplacement du kernel : https://deepai.org/machine-learning-glossary-and-terms/stride\n",
        "            padding = 2 #comme dans RSD mais pour le kernel\n",
        "        ),\n",
        "        nn.GELU(), #régression linéaire Gaussienne https://stackoverflow.com/questions/57532679/why-gelu-activation-function-is-used-instead-of-relu-in-bert\n",
        "        nn.MaxPool2d(kernel_size=2) # reformulation des données : https://www.geeksforgeeks.org/apply-a-2d-max-pooling-in-pytorch/\n",
        "    )\"\"\"\n",
        "\n",
        "    self.layerNorm = nn.LayerNorm(rnn_dim) #réduit les valeurs des paramètres de chaque neurone du réseau pour faciliter la descente de gradient : https://www.youtube.com/watch?v=TKPowx9fb-A\n",
        "\n",
        "    self.conv1 = self.sequence()\n",
        "    self.conv2 = self.sequence()\n",
        "    self.conv3 = self.sequence()\n",
        "    self.conv4 = self.sequence()\n",
        "    self.conv5 = self.sequence()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(rnn_dim*2, #taille retournée par BiLSTM\n",
        "                            rnn_dim)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def sequence(self) :\n",
        "    conv = nn.Sequential(\n",
        "      nn.LSTM(self.rnn_dim, #nombre de input initial\n",
        "              num_layers=self.n_lstm_layers, #nombre de couches passées à travers avant de retourner une valeur : https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
        "              hidden_size= self.rnn_dim, #nombre de composantes des vecteurs représentant les valeurs : https://stackoverflow.com/questions/75648914/trying-to-understand-lstm-parameter-hidden-size-in-pytorch#:~:text=The%20hidden_size%20is%20a%20hyper,hyper%2Dparameter%20(%20num_layers%20).\n",
        "              dropout=self.dropout, #réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "              bidirectional=True),\n",
        "      nn.GELU(), #régression linéaire Gaussienne https://stackoverflow.com/questions/57532679/why-gelu-activation-function-is-used-instead-of-relu-in-bert\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "  def forward(self, input_data): #traitement des données dans le réseau de neurone\n",
        "    print('*******************************************************')\n",
        "    print('input_data',input_data)\n",
        "    x = self.layerNorm(input_data)\n",
        "    print('*******************************************************1')\n",
        "    x = self.conv1(x) #RNN de 5 de profondeurs\n",
        "    print('*******************************************************2')\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv5(x)\n",
        "    #x = self.conv1(input_data)\n",
        "    ##passer le x des convolutional layers vers le flatten\n",
        "    #x = self.flatten(x)\n",
        "\n",
        "    logits = self.linear(x) #logits signifie la probabilité (avant d'être normalisé) associée à certaines réponses : https://www.linkedin.com/posts/mwitiderrick_what-are-logits-in-deep-learning-logits-activity-7084819307959902209-UUGe#:~:text=Logits%20are%20the%20outputs%20of%20a%20neural%20network%20before%20the,belonging%20to%20a%20certain%20class.\n",
        "    predictions = self.softmax(logits) #normaliser les logits\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "#algorithme pour le mapping prit et légèrement adapté depuis : https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "char_map_str = \"\"\"\n",
        " ’ 0\n",
        " ä 1\n",
        " a 2\n",
        " b 3\n",
        " c 4\n",
        " d 5\n",
        " e 6\n",
        " f 7\n",
        " g 8\n",
        " h 9\n",
        " i 10\n",
        " j 11\n",
        " k 12\n",
        " l 13\n",
        " m 14\n",
        " n 15\n",
        " o 16\n",
        " p 17\n",
        " q 18\n",
        " r 19\n",
        " s 20\n",
        " t 21\n",
        " u 22\n",
        " v 23\n",
        " w 24\n",
        " x 25\n",
        " y 26\n",
        " z 27\n",
        " - 28\n",
        " à 29\n",
        " â 30\n",
        " é 31\n",
        " è 32\n",
        " ê 33\n",
        " ë 34\n",
        " î 35\n",
        " ï 36\n",
        " ô 37\n",
        " ù 38\n",
        " û 39\n",
        " ü 40\n",
        " ÿ 41\n",
        " ç 42\n",
        " æ 43\n",
        " œ 44\n",
        " , 45\n",
        " . 46\n",
        " … 47\n",
        " ( 48\n",
        " ) 49\n",
        " « 50\n",
        " » 51\n",
        " ! 52\n",
        " ? 53\n",
        " \"\"\"\n",
        "#associer des caractères à des valeurs numériques\n",
        "class TextTransform:\n",
        "  \"\"\"Maps characters vers integers et vice versa\"\"\"\n",
        "  def __init__(self, char_map_str):\n",
        "      self.char_map_str = char_map_str\n",
        "      self.char_map = {}\n",
        "      self.index_map = {}\n",
        "      for line in char_map_str.strip().split('\\n'):\n",
        "          ch, index = line.split()\n",
        "          self.char_map[ch] = int(index)\n",
        "          self.index_map[int(index)] = ch\n",
        "      self.index_map[1] = ' '\n",
        "\n",
        "  def text_to_int(self, text):\n",
        "      \"\"\" Converti le texte en séquence de int à l'aide d'une map de texte à int \"\"\"\n",
        "      text = text.lower()\n",
        "      int_sequence = []\n",
        "      for c in text:\n",
        "          if c == ' ' or not(c in self.char_map_str):\n",
        "              ch = self.char_map['ä']#on utilise ä comme équivalent de ' '\n",
        "          else:\n",
        "              ch = self.char_map[c]\n",
        "          int_sequence.append(ch)\n",
        "      return int_sequence\n",
        "\n",
        "  def int_to_text(self, labels):\n",
        "      \"\"\" Converti une séquence de int en séquence de texte à l'aide d'une map de int à texte \"\"\"\n",
        "      string = []\n",
        "      for i in labels:\n",
        "          string.append(self.index_map[i])\n",
        "      return ''.join(string).replace('ä', ' ')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN81VZbWTGeK"
      },
      "source": [
        "# Entraîner et Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15ed70NTyntC",
        "outputId": "8238c0d3-3b43-49f8-be2d-d57aacfe8b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utiliation du processeur cpu\n",
            "*******************************************************\n",
            "input_data tensor([[[[3.1785e-09, 3.9616e-08, 1.5120e-09,  ..., 1.0056e-02,\n",
            "           3.7782e-02, 1.1121e-02],\n",
            "          [1.9522e-09, 8.5814e-08, 9.4201e-10,  ..., 1.8208e-02,\n",
            "           3.6617e-02, 7.1298e-03],\n",
            "          [1.4906e-10, 1.5992e-07, 5.0543e-10,  ..., 5.2291e-02,\n",
            "           7.5804e-02, 3.7157e-02],\n",
            "          ...,\n",
            "          [1.3983e-07, 2.3133e-07, 9.8039e-08,  ..., 4.7937e-02,\n",
            "           1.0756e+00, 1.4564e+01],\n",
            "          [6.5953e-08, 1.2409e-07, 1.2555e-07,  ..., 1.2069e-02,\n",
            "           1.5409e+00, 1.6203e+01],\n",
            "          [5.7427e-08, 9.4393e-08, 9.7188e-08,  ..., 1.5749e-02,\n",
            "           5.4747e+00, 3.1875e+01]]],\n",
            "\n",
            "\n",
            "        [[[1.0689e-07, 2.6586e-08, 2.1807e-07,  ..., 9.3331e-03,\n",
            "           2.0119e-02, 1.7282e+01],\n",
            "          [6.6276e-08, 5.6393e-08, 2.1335e-07,  ..., 9.8321e-02,\n",
            "           4.8184e-02, 2.4490e+01],\n",
            "          [4.2397e-08, 1.0048e-07, 2.1482e-07,  ..., 5.8308e-01,\n",
            "           1.8123e-01, 4.0016e+01],\n",
            "          ...,\n",
            "          [1.9289e-07, 2.5947e-07, 1.3856e-07,  ..., 5.5085e-02,\n",
            "           2.6625e-02, 4.9960e-02],\n",
            "          [3.5565e-08, 8.1330e-08, 2.2963e-07,  ..., 5.7607e-02,\n",
            "           7.5536e-03, 1.4839e-02],\n",
            "          [6.3845e-08, 4.4897e-08, 1.0608e-07,  ..., 4.2531e-02,\n",
            "           1.6762e-02, 2.3123e-02]]],\n",
            "\n",
            "\n",
            "        [[[1.2827e-09, 6.7456e-10, 4.2007e-10,  ..., 4.1510e-02,\n",
            "           1.3859e-04, 7.4849e-01],\n",
            "          [8.2357e-10, 1.1290e-09, 6.9672e-10,  ..., 2.8779e-02,\n",
            "           3.3876e-02, 1.4105e+00],\n",
            "          [3.8121e-10, 1.6027e-09, 1.0121e-09,  ..., 1.4706e-02,\n",
            "           1.5872e-01, 3.0276e+00],\n",
            "          ...,\n",
            "          [3.2052e-07, 1.5122e-07, 1.7818e-07,  ..., 5.9728e-01,\n",
            "           5.5517e-01, 1.0828e-04],\n",
            "          [1.2518e-07, 1.1966e-07, 1.4704e-07,  ..., 2.1409e-01,\n",
            "           7.2548e-01, 6.1499e-05],\n",
            "          [4.8859e-08, 1.3929e-07, 9.2947e-08,  ..., 1.5888e-01,\n",
            "           1.4632e-01, 3.5239e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[3.8704e-11, 7.9259e-07, 1.7147e-09,  ..., 1.5024e-02,\n",
            "           5.0355e-03, 8.4658e-01],\n",
            "          [2.6781e-11, 9.2268e-07, 1.1529e-09,  ..., 1.7599e-01,\n",
            "           7.3713e-03, 1.5847e+00],\n",
            "          [3.7230e-11, 1.1038e-06, 3.0135e-10,  ..., 4.2695e-01,\n",
            "           1.9318e+00, 2.4547e+00],\n",
            "          ...,\n",
            "          [1.4841e-07, 1.7664e-07, 2.3159e-07,  ..., 1.2453e-02,\n",
            "           1.4433e-01, 6.3600e-02],\n",
            "          [9.8723e-08, 1.0381e-07, 1.2455e-07,  ..., 2.5528e-02,\n",
            "           8.7906e-02, 5.8865e-02],\n",
            "          [7.5367e-08, 1.6235e-07, 1.3539e-07,  ..., 4.4913e-02,\n",
            "           1.7903e-01, 8.0194e-02]]],\n",
            "\n",
            "\n",
            "        [[[1.7846e-08, 4.1381e-08, 1.3631e-07,  ..., 2.1868e-03,\n",
            "           2.1410e-02, 3.7437e-01],\n",
            "          [1.2864e-08, 9.6736e-08, 1.3540e-07,  ..., 6.2627e-01,\n",
            "           1.4396e-01, 8.1120e-01],\n",
            "          [5.0700e-09, 1.8114e-07, 1.7174e-07,  ..., 2.0792e+00,\n",
            "           1.5899e+00, 2.1729e+00],\n",
            "          ...,\n",
            "          [1.0792e-07, 1.8734e-07, 3.3227e-07,  ..., 6.3510e-03,\n",
            "           5.7803e-05, 3.1503e-05],\n",
            "          [9.3358e-08, 1.8210e-07, 1.4164e-07,  ..., 3.5896e-03,\n",
            "           5.5721e-05, 5.3800e-05],\n",
            "          [7.0456e-08, 1.6099e-07, 9.4266e-08,  ..., 6.5111e-04,\n",
            "           7.9908e-05, 5.7843e-05]]],\n",
            "\n",
            "\n",
            "        [[[2.3210e-07, 2.9860e-08, 1.2016e-07,  ..., 5.3426e-03,\n",
            "           4.6352e-03, 2.9285e+00],\n",
            "          [1.7880e-07, 3.1694e-08, 2.3929e-07,  ..., 3.8283e-03,\n",
            "           8.0462e-02, 5.1208e+00],\n",
            "          [9.4308e-08, 3.3554e-08, 4.2402e-07,  ..., 6.5413e-02,\n",
            "           1.3035e+00, 8.7814e+00],\n",
            "          ...,\n",
            "          [2.7620e-07, 3.2808e-07, 3.3632e-07,  ..., 1.5988e-03,\n",
            "           3.1559e-04, 3.6057e-05],\n",
            "          [2.4759e-07, 1.4676e-07, 1.8583e-07,  ..., 1.1402e-03,\n",
            "           1.1694e-04, 4.5089e-05],\n",
            "          [6.4527e-08, 9.4094e-08, 9.8041e-08,  ..., 2.0796e-04,\n",
            "           9.3018e-05, 5.0667e-05]]]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given normalized_shape=[512], expected input with shape [*, 512], but got input of size[20, 1, 64, 44]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-eba0b4cc6910>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m   \u001b[0mentrainerEpoque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesseur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mtester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesseur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexte_transforme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_caracteres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-eba0b4cc6910>\u001b[0m in \u001b[0;36mentrainerEpoque\u001b[0;34m(modele, chargeurDonnees, criterion, optimizer, scheduler, epoch, processeur)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#faire une prédiction et déterminer de comment il faut modifier notre réseau de neurone, à l'aide du CTCLoss, pour améliorer la prédiction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodele\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdivergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextesAssocies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-2a60d81ae206>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*******************************************************'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*******************************************************1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#RNN de 5 de profondeurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    202\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2544\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         )\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[512], expected input with shape [*, 512], but got input of size[20, 1, 64, 44]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#paramètres essentiels lors de machine learning\n",
        "hyper_parameters = { #paramètres qui décident comment se déroulera l'entrainement : https://aws.amazon.com/fr/what-is/hyperparameter-tuning/\n",
        "        \"n_lstm_layers\": 2, #nombre de couches de lstm\n",
        "        \"rnn_dim\": 512, #inChannels\n",
        "        \"n_caracteres\": 54, #dimensionOutput\n",
        "        \"dropout\": 0.1, ##réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "        \"learning_rate\": 0.001, #vitesse d'apprentissage\n",
        "        \"batch_size\": 20, #nombre d'éléments par endoit qu'on entraine\n",
        "        \"epochs\": 10 #nombre de fois qu'on entraine le réseau au complet\n",
        "    }\n",
        "\n",
        "\"\"\"entrainer le RNN à l'aide du CTC Algorithm, un algorithme qui sert à déterminer où sont placer les lettres dans un fichier audio : https://ketanhdoshi.github.io/Audio-ASR/\"\"\"\n",
        "\n",
        "\n",
        "def entrainerEpoque(modele, chargeurDonnees, criterion, optimizer, scheduler, epoch, processeur):\n",
        "  for audios, textesAssocies in chargeurDonnees :\n",
        "    #s'assurer que l'audio et le texte soit au même endroit que le modèle\n",
        "    audios = audios.to(processeur)\n",
        "    textesAssocies = textesAssocies .to(processeur)\n",
        "\n",
        "    #réinitialise les paramètres pour permettre au réseau de neurone de ne pas s'encombrer de paramètres précédents inutiles : https://medium.com/@lazyprogrammerofficial/in-pytorch-why-do-we-need-to-call-optimizer-zero-grad-8e19fdc1ad2f\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #faire une prédiction et déterminer de comment il faut modifier notre réseau de neurone, à l'aide du CTCLoss, pour améliorer la prédiction\n",
        "    prediction = modele(audios)\n",
        "    divergence = criterion(audios, textesAssocies)\n",
        "\n",
        "    #update les valeurs du réseau de neurone avec une backpropagation\n",
        "    divergence.backward() # modifie le poid de chaque paramètres selon la divergence calculée (est-ce qu'il faut augmenter ou diminuer la valeur de ce neurone) : https://en.wikipedia.org/wiki/Backpropagation\n",
        "\n",
        "\n",
        "    #passage au prochain paramètre de l'optimizer et du scheduler\n",
        "    optimizer.step() #descente de gradient : https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step\n",
        "    scheduler.step() #modifie le learning rate : https://discuss.pytorch.org/t/what-does-scheduler-step-do/47764\n",
        "\n",
        "def tester(modele, chargeurDonnees, criterion, epoch, processeur, texte_transforme, caractere_vide) :\n",
        "\n",
        "  modele.eval() # rend fixe certains paramètres qui sont changés durant l'entraînement : https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
        "  divergence_test = 0\n",
        "  erreurLettre = 0\n",
        "  nombreLettre = 0\n",
        "\n",
        "  with torch.no_grad() : #même chose que model.eval() sur d'autres paramètres\n",
        "    for audio, texteAssocie in chargeurDonnees :\n",
        "      #code similaire au train\n",
        "      audio = audio.to(processeur)\n",
        "      texteAssocie = texteAssocie\n",
        "\n",
        "      prediction = modele(audio)\n",
        "      divergence = criterion(audio, texteAssocie)\n",
        "      divergence_test += divergence\n",
        "\n",
        "      #transformer les valeurs du spectogram en valeurs numériques et enuite en texte\n",
        "      predictions_decode, valeurs_decode = GreedyDecoder(prediction, texteAssocie, texte_transforme, caractere_vide)\n",
        "      nombreLettre += max(len(predictions_decode), len(valeurs_decode))\n",
        "      for i in range(len(predictions_decode)) :\n",
        "        if i < valeurs_decode :\n",
        "          if valeurs_decode != predictions_decode :\n",
        "            erreurLettre += 1\n",
        "\n",
        "      pourcentErreur = erreurLettre/nombreLettre*100\n",
        "      print(f\"Le pourcentage d'erreur à l'époque {epoch} = {pourcentErreur}% pour les lettres et la divergence(loss) = {divergence_test}\")\n",
        "\n",
        "\"\"\"\n",
        "fonction courante dans les speech to text qui compare le résultat attendu avec le résultat obtenu\n",
        "le format de celui-ci est inspiré par : https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "\"\"\"\n",
        "def GreedyDecoder(prediction, texteAssocie, texte_transforme, caractere_vide) :\n",
        "  valeursPrevuesRNN = torch.argmax(prediction) #retourne les valeurs du RNN les plus probables (maximum) #https://pytorch.org/docs/stable/generated/torch.argmax.html\n",
        "  decode = []\n",
        "  attendu = texteAssocie.split(\" \")\n",
        "  for i, valeurs in enumerate(valeursPrevuesRNN) : #valeurs correspond au caracère à chaque endroit possible\n",
        "    for j, index in enumerate(valeurs) : #valeur correspond au caractère à un endroit précis\n",
        "      if index != caractere_vide:\n",
        "        decode.append(index)\n",
        "  decode = texte_transforme.int_to_text(decode)\n",
        "  return decode, valeursPrevuesRNN\n",
        "\n",
        "\n",
        "FICHIER_ANNOTE = \"/content/drive/MyDrive/ColabNotebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_text.list\"\n",
        "FICHIER_AUDIO = \"/content/drive/MyDrive/ColabNotebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_wavs.list\"\n",
        "\n",
        "\n",
        "#nombre d'échantillons par secondes dans notre audio\n",
        "SAMPLE_RATE = 22050\n",
        "NOMBRE_ECHANTILLONS = 22050\n",
        "\n",
        "if torch.cuda.is_available(): #détermine ce qui exécute le programme (gpu préférable pour AI audio)\n",
        "    processeur = \"cuda\"\n",
        "else:\n",
        "    processeur = \"cpu\"\n",
        "print(f\"Utiliation du processeur {processeur}\")\n",
        "\n",
        "#le Spectogram de Mel est une échelle logarithmique utilisée pour mieux représenter les différences qu'un humain entend dans un fichier audio ce qui aide à l'analyse sonore\n",
        "melSpectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    SAMPLE_RATE,\n",
        "    n_fft=512, #longueur physique du signal optimale pour la reconnaissance vocale selon : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    hop_length=512, #nombre d'échantillon audio adjacents analysés par la transformée de fourier : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    n_mels=64 #nombre de séparations d'une seule fréquence optimale pour la reconnaisance vocale selon:https://stackoverflow.com/questions/62623975/why-128-mel-bands-are-used-in-mel-spectrograms\n",
        ")\n",
        "\n",
        "texte_transforme = TextTransform(char_map_str)\n",
        "\n",
        "#initialisation du Dataset\n",
        "rvd = ReconnaissanceVocaleDataset(FICHIER_ANNOTE, FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur, texte_transforme)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "train_data_loader = DataLoader(rvd,\n",
        "                              hyper_parameters[\"batch_size\"],\n",
        "                              shuffle=True, #https://discuss.pytorch.org/t/how-does-shuffle-in-data-loader-work/49756/7\n",
        "                              )\n",
        "test_data_loader = DataLoader(rvd, #préférablement pas tt le dataset(rvd)\n",
        "                              hyper_parameters[\"batch_size\"],\n",
        "                              shuffle=False,\n",
        "                              )\n",
        "\n",
        "#initialisation du réseau de neurones\n",
        "rnn = BiRNNetwork(hyper_parameters[\"n_caracteres\"], hyper_parameters[\"rnn_dim\"], hyper_parameters[\"n_lstm_layers\"], hyper_parameters[\"dropout\"])\n",
        "rnn = rnn.to(processeur) #to(processeur) s'assure que tout s'entraine au même endroit (sur le cuda dans ce cas ci)\n",
        "\n",
        "#le choix du optimizer et du scheduler a été effectué selon l'article : https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "\"\"\"\n",
        "On utilise le CTCLoss function dans le speech to text pour aligner les endroits où il est prédit qu'il y ait des lettres avec les bons neurones (Doshi, 2021)\n",
        "blank permet de ne pas tenir compte des endroits où on prédit qu'il n'y aura pas de caractères : https://distill.pub/2017/ctc/?undefined=&ref=assemblyai.com\n",
        "\"\"\"\n",
        "criterion = nn.CTCLoss(blank = hyper_parameters[\"n_caracteres\"]).to(processeur) #calcul les probabilités selon une fonction prédéfinie : https://nn.readthedocs.io/en/rtd/criterion/index.html\n",
        "\n",
        "optimizer = torch.optim.AdamW(rnn.parameters(), hyper_parameters[\"learning_rate\"]) #change les paramètres du modèle pour améliorer la performance : https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hyper_parameters[\"learning_rate\"],\n",
        "                                                epochs=hyper_parameters[\"epochs\"], # paramètres nécessaires au scheduler :https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\n",
        "                                                steps_per_epoch=len(rvd) # nombre de neurones qu'on entraîne par epoch : https://community.deeplearning.ai/t/request-for-explanation-on-steps-per-epoch-parameter/501777\n",
        "                                                ) #modifie le learning rate pour améliorer la performance : https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90\n",
        "\n",
        "\n",
        "for epoch in range(hyper_parameters[\"epochs\"]):\n",
        "  entrainerEpoque(rnn, train_data_loader, criterion, optimizer, scheduler, epoch, processeur)\n",
        "  if epoch % 2 == 0 :\n",
        "    tester(rnn, test_data_loader, criterion, epoch, processeur, texte_transforme, hyper_parameters[\"n_caracteres\"])\n",
        "\n",
        "\n",
        "torch.save(rnn.state_dict(), \"feedforwardnet.pth\") #sauvegarder le modele : https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
        "print(\"RNN entraîné sauvegardé sur feedforwardnet.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72q5Kr3CTMyK"
      },
      "source": [
        "# Inférence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzWcdVSdypJD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}