{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex64-1149/VoxNote/blob/IA-VoxNote/VoxNote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptation des données"
      ],
      "metadata": {
        "id": "zeFHx1nkShwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#adaptation de https://github.com/musikalkemist/pytorchforaudio\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class ReconnaissanceVocaleDataset(Dataset) :\n",
        "\n",
        "  #Initialise les aurguments de la classe dataset:\n",
        "  # 1.fichier annotation(fichier txt qui contient tous les noms des fichiers audio (wav))\n",
        "  # 2.fichier audio ( contient tous les fichiers audio(wav))\n",
        "  def __init__(self,FICHIER_ANNOTE,FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur):\n",
        "    self.fichierAnnotations=pd.read_csv(FICHIER_ANNOTE)\n",
        "    self.fichierAudio=FICHIER_AUDIO\n",
        "    self.processeur = processeur\n",
        "    self.melSpectrogram = melSpectrogram.to(processeur) #s'assurer que tout se fasse au même endroit dans l'ordinateur : https://stackoverflow.com/questions/63061779/pytorch-when-do-i-need-to-use-todevice-on-a-model-or-tensor\n",
        "    self.SAMPLE_RATE_VOULU = SAMPLE_RATE\n",
        "    self.NOMBRE_ECHANTILLONS = NOMBRE_ECHANTILLONS\n",
        "\n",
        "  #retourne le nombre de fichiers dans notre dataset\n",
        "  def __len__(self):\n",
        "    return len(self.fichierAnnotations)\n",
        "\n",
        "  #retourne l'audio ainsi que son fichier texte associé\n",
        "  def __getitem__(self, index):\n",
        "    pathAudio = self.getAudioSamplePath(index)\n",
        "    texte = self.getAudioSampleText(index)\n",
        "    #signal informatique et sample rate de notre audio\n",
        "    signal, sr = torchaudio.load(pathAudio)\n",
        "\n",
        "    #mettre le signal au même endroit que sa transformation\n",
        "    signal = signal.to(processeur)\n",
        "\n",
        "    #normaliser l'audio\n",
        "    signal = self.resampleSiNecessaire(signal, sr)\n",
        "    signal = self.combinerSiNecessaire(signal)\n",
        "    #diminuer ou ajouter des échantillons \"vides\" si le nombre d'échantillons ne correspond à 22 050\n",
        "    signal = self.couperSiNecessaire(signal)\n",
        "    signal = self.paddingSiNecessaire(signal)\n",
        "\n",
        "    #transformer l'audio dans le spectogram de mel\n",
        "    signal = self.melSpectrogram(signal)\n",
        "    return signal, texte\n",
        "\n",
        "  #mettre tous les fichiers audios à la même fréquence d'échantillonage\n",
        "  def resampleSiNecessaire(self, signal, sr):\n",
        "    if sr != self.SAMPLE_RATE_VOULU :\n",
        "      resampler = torchaudio.transforms.Resample(sr, self.SAMPLE_RATE_VOULU)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  #s'assurer que l'audio ne contient qu'une entrée et sortie (que le son ne soit pas stéréo) pour le normaliser\n",
        "  def combinerSiNecessaire(self, signal):\n",
        "    if signal.shape[0] > 1 :\n",
        "      signal = torch.mean(signal, dim=0, keepdim= True)\n",
        "    return signal\n",
        "\n",
        "  #enlever les échantillons audios superflus\n",
        "  def couperSiNecessaire(self, signal) :\n",
        "    #le signal est composé de 2 dimensions : [nombre de source du signal(1 dans ce cas), longueur du signal – nombre d'échantillons (on veut 22 050)]\n",
        "    if signal.shape[1] > self.NOMBRE_ECHANTILLONS :\n",
        "      signal = signal[:, :self.NOMBRE_ECHANTILLONS] #utilité de [:, :] : la première dimension est prise au complet et la deuxième jusqu'à l'atteinte du nombre d'échantillon (expliquer à https://youtu.be/WyJvrzVNkOc?list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&t=478)\n",
        "    return signal\n",
        "\n",
        "  def paddingSiNecessaire(self, signal) :\n",
        "    #la longueur du signal = signal.shape[1] comme expliqué précédemment\n",
        "    if signal.shape[1] < self.NOMBRE_ECHANTILLONS :\n",
        "      paddingDuSignal = (0, self.NOMBRE_ECHANTILLONS - signal.shape[1]) #(0, nombre d'échantillons manquants)\n",
        "      torch.nn.functional.pad(signal, paddingDuSignal) #ajoute le padding au signal (https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  #retourne le chemin pour avoir le bon fichier audio à un certain index du FICHIER_ANNOTE\n",
        "  def getAudioSamplePath(self, index):\n",
        "    return self.fichierAudio[index]\n",
        "\n",
        "  #retourne le fichier texte associé à l'audio d'un certain index du FICHIER_AUDIO\n",
        "  def getAudioSampleText(self, index):\n",
        "    return self.fichierAnnotations[index]\n",
        "\n",
        "\n",
        "FICHIER_ANNOTE = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_text.list\"\n",
        "FICHIER_AUDIO = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_wavs.list\"\n",
        "\n",
        "\n",
        "#nombre d'échantillons par secondes dans notre audio\n",
        "SAMPLE_RATE = 22050\n",
        "NOMBRE_ECHANTILLONS = 22050\n",
        "\n",
        "if torch.cuda.is_available(): #détermine ce qui exécute le programme (gpu préférable pour AI audio)\n",
        "    processeur = \"cuda\"\n",
        "else:\n",
        "    processeur = \"cpu\"\n",
        "print(f\"Utiliation du processeur {processeur}\")\n",
        "\n",
        "#le Spectogram de Mel est une échelle logarithmique utilisée pour mieux représenter les différences qu'un humain entend dans un fichier audio ce qui aide à l'analyse sonore\n",
        "melSpectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    SAMPLE_RATE,\n",
        "    n_fft=512, #longueur physique du signal optimale pour la reconnaissance vocale selon : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    hop_length=512, #nombre d'échantillon audio adjacents analysés par la transformée de fourier : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    n_mels=64 #nombre de séparations d'une seule fréquence optimale pour la reconnaisance vocale selon:https://stackoverflow.com/questions/62623975/why-128-mel-bands-are-used-in-mel-spectrograms\n",
        ")\n",
        "\n",
        "\n",
        "rvd = ReconnaissanceVocaleDataset(FICHIER_ANNOTE, FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur)\n"
      ],
      "metadata": {
        "id": "a4QSC_m-8aL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133d9792-d620-43a4-ecde-74dc86ab2d41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utiliation du cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #nécessaire qu début de chaque session\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL25QMQ9YtRV",
        "outputId": "17f706ec-fed2-42dc-af49-0a18b295244f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#utilisé pour extraire le zip de google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#!unzip file.zip\n"
      ],
      "metadata": {
        "id": "iSiu0ku4h3lz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Réseau de neurone"
      ],
      "metadata": {
        "id": "ydEmnsTDS43i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#connaissances nécessaires au CNN trouvées à l'adresse suivante : https://ketanhdoshi.github.io/Audio-ASR/\n",
        "#CNN initial vient de https://www.youtube.com/watch?v=SQ1iIKs190Q&list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&index=8\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "!torch -m pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "class CNNNEtwork(nn.Module) :\n",
        "  def __init__(self, nbCaracteresPossible) :\n",
        "    super().__init__()\n",
        "    outChannelsInitial = 128\n",
        "    # 4 conv blocks / flatten / linear /softmax\n",
        "    \"\"\"\n",
        "    1. Créer les 4 blocs du CNN\n",
        "    2. 'flatten' le résultat en diminuant le nombre de dimensions créées avec les blocs du CNN\n",
        "    3. transformer en équation linéaire les données fournies : https://docs.kanaries.net/topics/Python/nn-linear\n",
        "    4. normaliser les résultats à l'aide de softmax\n",
        "    \"\"\"\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d( #couche en 2 dimensions\n",
        "            in_channels=1, #nombre de input initial (=1 lors de l'adaptation des données)\n",
        "            out_channels=outChannelsInitial, #nombre de filtre dans cette couche du réseau de neurone\n",
        "            kernel_size=3, #nombre de choses analysées en même temps : https://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean\n",
        "            stride = 1, #déplacement du kernel : https://deepai.org/machine-learning-glossary-and-terms/stride\n",
        "            padding = 2 #comme dans RSD mais pour le kernel\n",
        "        ),\n",
        "        nn.ReLU(), #régression linéaire\n",
        "        nn.MaxPool2d(kernel_size=2) # reformulation des données : https://www.geeksforgeeks.org/apply-a-2d-max-pooling-in-pytorch/\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=outChannelsInitial, # = au out_channels précédent\n",
        "            out_channels=outChannelsInitial*2, # out_channels précédent *2\n",
        "            kernel_size=3,\n",
        "            stride = 1,\n",
        "            padding = 2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=outChannelsInitial*2,\n",
        "            out_channels=outChannelsInitial*4,\n",
        "            kernel_size=3,\n",
        "            stride = 1,\n",
        "            padding = 2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=outChannelsInitial*4,\n",
        "            out_channels=outChannelsInitial*8,\n",
        "            kernel_size=3,\n",
        "            stride = 1,\n",
        "            padding = 2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(outChannelsInitial*8 *5 * 4, #input (out_channels final, fréquence, temps)\n",
        "                            nbCaracteresPossible #nb de output\n",
        "                            )\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_data): #envoyer les données d'une couche à une autre\n",
        "    x = self.conv1(input_data)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    #passer le x des convolutional layers vers le flatten\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear(x) #logits signifie la probabilité (avant d'être normaliser) associée à certaines réponses : https://www.linkedin.com/posts/mwitiderrick_what-are-logits-in-deep-learning-logits-activity-7084819307959902209-UUGe#:~:text=Logits%20are%20the%20outputs%20of%20a%20neural%20network%20before%20the,belonging%20to%20a%20certain%20class.\n",
        "    predictions = self.softmax(logits) #normaliser les logits\n",
        "    return predictions\n",
        "#algorithme pour le mapping prit sur https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "char_map_str = \"\"\"\n",
        " ' 0\n",
        " <SPACE> 1\n",
        " a 2\n",
        " b 3\n",
        " c 4\n",
        " d 5\n",
        " e 6\n",
        " f 7\n",
        " g 8\n",
        " h 9\n",
        " i 10\n",
        " j 11\n",
        " k 12\n",
        " l 13\n",
        " m 14\n",
        " n 15\n",
        " o 16\n",
        " p 17\n",
        " q 18\n",
        " r 19\n",
        " s 20\n",
        " t 21\n",
        " u 22\n",
        " v 23\n",
        " w 24\n",
        " x 25\n",
        " y 26\n",
        " z 27\n",
        " - 28\n",
        " <EMP> 29\n",
        " à 30\n",
        " â 31\n",
        " ä 32\n",
        " é 33\n",
        " è 34\n",
        " ê 35\n",
        " ë 36\n",
        " î 37\n",
        " ï 38\n",
        " ô 39\n",
        " ö 40\n",
        " ù 41\n",
        " û 42\n",
        " ü 43\n",
        " ÿ 44\n",
        " ç 45\n",
        " \"\"\"\n",
        "#associer des caractères à des valeurs numériques\n",
        "class TextTransform:\n",
        "  \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "  def __init__(self):\n",
        "      char_map_str = char_map_str\n",
        "      self.char_map = {}\n",
        "      self.index_map = {}\n",
        "      for line in char_map_str.strip().split('\\n'):\n",
        "          ch, index = line.split()\n",
        "          self.char_map[ch] = int(index)\n",
        "          self.index_map[int(index)] = ch\n",
        "      self.index_map[1] = ' '\n",
        "\n",
        "  def text_to_int(self, text):\n",
        "      \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "      int_sequence = []\n",
        "      for c in text:\n",
        "          if c == ' ':\n",
        "              ch = self.char_map['']\n",
        "          else:\n",
        "              ch = self.char_map[c]\n",
        "          int_sequence.append(ch)\n",
        "      return int_sequence\n",
        "\n",
        "  def int_to_text(self, labels):\n",
        "      \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "      string = []\n",
        "      for i in labels:\n",
        "          string.append(self.index_map[i])\n",
        "      return ''.join(string).replace('', ' ')\n",
        "\n",
        "nombreCaracteres = 46\n",
        "\n",
        "cnn = CNNNEtwork(nombreCaracteres)\n",
        "summary(cnn.cuda(), (1,64,44)) #(nombre de channels, frequence, temps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFqcIJnIc4ZK",
        "outputId": "5ede0811-9c1c-401a-988d-27eee210363a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: torch: command not found\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 128, 66, 46]           1,280\n",
            "              ReLU-2          [-1, 128, 66, 46]               0\n",
            "         MaxPool2d-3          [-1, 128, 33, 23]               0\n",
            "            Conv2d-4          [-1, 256, 35, 25]         295,168\n",
            "              ReLU-5          [-1, 256, 35, 25]               0\n",
            "         MaxPool2d-6          [-1, 256, 17, 12]               0\n",
            "            Conv2d-7          [-1, 512, 19, 14]       1,180,160\n",
            "              ReLU-8          [-1, 512, 19, 14]               0\n",
            "         MaxPool2d-9            [-1, 512, 9, 7]               0\n",
            "           Conv2d-10          [-1, 1024, 11, 9]       4,719,616\n",
            "             ReLU-11          [-1, 1024, 11, 9]               0\n",
            "        MaxPool2d-12           [-1, 1024, 5, 4]               0\n",
            "          Flatten-13                [-1, 20480]               0\n",
            "           Linear-14                   [-1, 46]         942,126\n",
            "          Softmax-15                   [-1, 46]               0\n",
            "================================================================\n",
            "Total params: 7,138,350\n",
            "Trainable params: 7,138,350\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 14.67\n",
            "Params size (MB): 27.23\n",
            "Estimated Total Size (MB): 41.91\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraîner"
      ],
      "metadata": {
        "id": "QN81VZbWTGeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tester\n"
      ],
      "metadata": {
        "id": "-derVaPjTKNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inférence"
      ],
      "metadata": {
        "id": "72q5Kr3CTMyK"
      }
    }
  ]
}