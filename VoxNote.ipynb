{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex64-1149/VoxNote/blob/IA-VoxNote/VoxNote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFHx1nkShwn"
      },
      "source": [
        "# Adaptation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "a4QSC_m-8aL-"
      },
      "outputs": [],
      "source": [
        "\n",
        "#adaptation de https://github.com/musikalkemist/pytorchforaudio\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class ReconnaissanceVocaleDataset(Dataset) :\n",
        "\n",
        "  #Initialise les aurguments de la classe dataset:\n",
        "  # 1.fichier annotation(fichier txt qui contient tous les noms des fichiers audio (wav))\n",
        "  # 2.fichier audio ( contient tous les fichiers audio(wav))\n",
        "  def __init__(self,FICHIER_ANNOTE,FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur): # __[...]__ = classe nécessaire a un dataset Pytorch\n",
        "    self.fichierAnnotations=FICHIER_ANNOTE\n",
        "    self.fichierAudio=FICHIER_AUDIO\n",
        "    self.processeur = processeur\n",
        "    self.melSpectrogram = melSpectrogram.to(processeur) #s'assurer que tout se fasse au même endroit dans l'ordinateur : https://stackoverflow.com/questions/63061779/pytorch-when-do-i-need-to-use-todevice-on-a-model-or-tensor\n",
        "    self.SAMPLE_RATE_VOULU = SAMPLE_RATE\n",
        "    self.NOMBRE_ECHANTILLONS = NOMBRE_ECHANTILLONS\n",
        "\n",
        "  #retourne le nombre de fichiers dans notre dataset\n",
        "  def __len__(self):\n",
        "    return len(self.fichierAnnotations)\n",
        "\n",
        "  #retourne l'audio ainsi que son fichier texte associé\n",
        "  def __getitem__(self, index):\n",
        "    pathAudio = self.getAudioSamplePath(index)\n",
        "    texte = self.getAudioSampleText(index)\n",
        "    #signal informatique et sample rate de notre audio\n",
        "    signal, sr = torchaudio.load(pathAudio)\n",
        "\n",
        "    #mettre le signal au même endroit que sa transformation\n",
        "    signal = signal.to(self.processeur)\n",
        "\n",
        "    #normaliser l'audio\n",
        "    signal = self.resampleSiNecessaire(signal, sr)\n",
        "    signal = self.combinerSiNecessaire(signal)\n",
        "    #diminuer ou ajouter des échantillons \"vides\" si le nombre d'échantillons ne correspond à 22 050\n",
        "    signal = self.couperSiNecessaire(signal)\n",
        "    signal = self.paddingSiNecessaire(signal)\n",
        "\n",
        "    #transformer l'audio dans le spectogram de mel\n",
        "    signal = self.melSpectrogram(signal)\n",
        "    return signal, texte\n",
        "\n",
        "  #mettre tous les fichiers audios à la même fréquence d'échantillonage\n",
        "  def resampleSiNecessaire(self, signal, sr):\n",
        "    if sr != self.SAMPLE_RATE_VOULU :\n",
        "      resampler = torchaudio.transforms.Resample(sr, self.SAMPLE_RATE_VOULU)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  #s'assurer que l'audio ne contient qu'une entrée et sortie (que le son ne soit pas stéréo) pour le normaliser\n",
        "  def combinerSiNecessaire(self, signal):\n",
        "    if signal.shape[0] > 1 :\n",
        "      signal = torch.mean(signal, dim=0, keepdim= True)\n",
        "    return signal\n",
        "\n",
        "  #enlever les échantillons audios superflus\n",
        "  def couperSiNecessaire(self, signal) :\n",
        "    #le signal est composé de 2 dimensions : [nombre de source du signal(1 dans ce cas), longueur du signal – nombre d'échantillons (on veut 22 050)]\n",
        "    if signal.shape[1] > self.NOMBRE_ECHANTILLONS :\n",
        "      signal = signal[:, :self.NOMBRE_ECHANTILLONS] #utilité de [:, :] : la première dimension est prise au complet et la deuxième jusqu'à l'atteinte du nombre d'échantillon (expliquer à https://youtu.be/WyJvrzVNkOc?list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&t=478)\n",
        "    return signal\n",
        "\n",
        "  def paddingSiNecessaire(self, signal) :\n",
        "    #la longueur du signal = signal.shape[1] comme expliqué précédemment\n",
        "    if signal.shape[1] < self.NOMBRE_ECHANTILLONS :\n",
        "      paddingDuSignal = (0, self.NOMBRE_ECHANTILLONS - signal.shape[1]) #(0, nombre d'échantillons manquants)\n",
        "      torch.nn.functional.pad(signal, paddingDuSignal) #ajoute le padding au signal (https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  # TODO: vérifier fichierAurio[index] renvoie quoi\n",
        "  #retourne le chemin pour avoir le bon fichier audio à un certain index du FICHIER_ANNOTE\n",
        "  def getAudioSamplePath(self, index):\n",
        "    return self.fichierAudio[index]\n",
        "\n",
        "  #retourne le fichier texte associé à l'audio d'un certain index du FICHIER_AUDIO\n",
        "  def getAudioSampleText(self, index):\n",
        "    return self.fichierAnnotations[index]\n",
        "\n",
        "\n",
        "#FICHIER_ANNOTE = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_text.list\"\n",
        "#FICHIER_AUDIO = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_wavs.list\"\n",
        "#\n",
        "#\n",
        "##nombre d'échantillons par secondes dans notre audio\n",
        "#SAMPLE_RATE = 22050\n",
        "#NOMBRE_ECHANTILLONS = 22050\n",
        "#\n",
        "#if torch.cuda.is_available(): #détermine ce qui exécute le programme (gpu préférable pour AI audio)\n",
        "#    processeur = \"cuda\"\n",
        "#else:\n",
        "#    processeur = \"cpu\"\n",
        "#print(f\"Utiliation du processeur {processeur}\")\n",
        "#\n",
        "##le Spectogram de Mel est une échelle logarithmique utilisée pour mieux représenter les différences qu'un humain entend dans un fichier audio ce qui aide à l'analyse sonore\n",
        "#melSpectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "#    SAMPLE_RATE,\n",
        "#    n_fft=512, #longueur physique du signal optimale pour la reconnaissance vocale selon : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "#    hop_length=512, #nombre d'échantillon audio adjacents analysés par la transformée de fourier : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "#    n_mels=64 #nombre de séparations d'une seule fréquence optimale pour la reconnaisance vocale selon:https://stackoverflow.com/questions/62623975/why-128-mel-bands-are-used-in-mel-spectrograms\n",
        "#)\n",
        "#\n",
        "#\n",
        "#rvd = ReconnaissanceVocaleDataset(FICHIER_ANNOTE, FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL25QMQ9YtRV",
        "outputId": "65f21d60-9bc9-45a0-fcd3-e55eae4bf47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive #nécessaire qu début de chaque session\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "iSiu0ku4h3lz"
      },
      "outputs": [],
      "source": [
        "#utilisé pour extraire le zip de google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#!unzip file.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydEmnsTDS43i"
      },
      "source": [
        "# Réseau de neurone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFqcIJnIc4ZK",
        "outputId": "1ee51754-9a99-4521-a6b2-6b948d39f53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: torch: command not found\n"
          ]
        }
      ],
      "source": [
        "#connaissances nécessaires au RNN(LSTM) trouvées à l'adresse suivante : https://ketanhdoshi.github.io/Audio-ASR/\n",
        "#CNN initial vient de https://www.youtube.com/watch?v=SQ1iIKs190Q&list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&index=8\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "!torch -m pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "class BiRNNetwork(nn.Module) : #RNN de type bidirectional LSTM\n",
        "\n",
        "  def __init__(self, n_caracteres, rnn_dim, n_lstm_layers, dropout) :\n",
        "    super().__init__()\n",
        "    # 5 RNN blocks / flatten / linear /softmax\n",
        "    \"\"\"\n",
        "    1. Créer le bi-lstm (une sous catégorie des RNN) : https://medium.com/@anishnama20/understanding-bidirectional-lstm-for-sequential-data-processing-b83d6283befc\n",
        "    2. 'flatten' le résultat en diminuant le nombre de dimensions créées avec les blocs du RNN\n",
        "    3. transformer en équation linéaire les données fournies : https://docs.kanaries.net/topics/Python/nn-linear\n",
        "    4. normaliser les résultats à l'aide de softmax\n",
        "    \"\"\"\n",
        "    self.n_caracteres = n_caracteres\n",
        "    self.rnn_dim = rnn_dim\n",
        "    self.n_lstm_layers = n_lstm_layers\n",
        "    self.dropout = dropout\n",
        "    #self.batchSize = batchSize #nombre d'échantillon avant de changer les paramètres : https://datascience.stackexchange.com/questions/36651/relationship-between-batch-size-and-the-number-of-neurons-in-the-input-layer\n",
        "\n",
        "\n",
        "    \"\"\"self.conv1 = nn.Sequential(\n",
        "        nn.Dropout(0.1),#réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "        nn.Conv2d( #couche en 2 dimensions\n",
        "            in_channels=inChannels, #nombre de input initial (=1 lors de l'adaptation des données)\n",
        "            out_channels=outChannels, #nombre de filtre dans cette couche du réseau de neurone\n",
        "            kernel_size=3, #nombre de choses analysées en même temps : https://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean\n",
        "            stride = 1, #déplacement du kernel : https://deepai.org/machine-learning-glossary-and-terms/stride\n",
        "            padding = 2 #comme dans RSD mais pour le kernel\n",
        "        ),\n",
        "        nn.GELU(), #régression linéaire Gaussienne https://stackoverflow.com/questions/57532679/why-gelu-activation-function-is-used-instead-of-relu-in-bert\n",
        "        nn.MaxPool2d(kernel_size=2) # reformulation des données : https://www.geeksforgeeks.org/apply-a-2d-max-pooling-in-pytorch/\n",
        "    )\"\"\"\n",
        "\n",
        "    self.layerNorm = nn.LayerNorm(rnn_dim) #réduit les valeurs des paramètres de chaque neurone du réseau pour faciliter la descente de gradient : https://www.youtube.com/watch?v=TKPowx9fb-A\n",
        "\n",
        "    self.conv1 = self.sequence()\n",
        "    self.conv2 = self.sequence()\n",
        "    self.conv3 = self.sequence()\n",
        "    self.conv4 = self.sequence()\n",
        "    self.conv5 = self.sequence()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(n_caracteres *5 * 2, #input (out_channels final, fréquence, temps)\n",
        "                           n_caracteres #nb de output\n",
        "                           )\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def sequence(self) :\n",
        "    conv = nn.Sequential(\n",
        "      nn.LSTM(self.rnn_dim, #nombre de input initial\n",
        "              num_layers=self.n_lstm_layers, #nombre de couches passées à travers avant de retourner une valeur : https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
        "              hidden_size= self.rnn_dim, #nombre de composantes des vecteurs représentant les valeurs : https://stackoverflow.com/questions/75648914/trying-to-understand-lstm-parameter-hidden-size-in-pytorch#:~:text=The%20hidden_size%20is%20a%20hyper,hyper%2Dparameter%20(%20num_layers%20).\n",
        "              dropout=self.dropout, #réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "              bidirectional=True),\n",
        "      nn.GELU(), #régression linéaire Gaussienne https://stackoverflow.com/questions/57532679/why-gelu-activation-function-is-used-instead-of-relu-in-bert\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "  def forward(self, input_data): #traitement des données dans le réseau de neurone\n",
        "    x = self.layerNorm(input_data)\n",
        "    x = self.conv1(x) #RNN de 5 de profondeurs\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.layerNorm(x)\n",
        "    x = self.conv5(x)\n",
        "    #x = self.conv1(input_data)\n",
        "    ##passer le x des convolutional layers vers le flatten\n",
        "    #x = self.flatten(x)\n",
        "\n",
        "    logits = self.linear(x) #logits signifie la probabilité (avant d'être normalisé) associée à certaines réponses : https://www.linkedin.com/posts/mwitiderrick_what-are-logits-in-deep-learning-logits-activity-7084819307959902209-UUGe#:~:text=Logits%20are%20the%20outputs%20of%20a%20neural%20network%20before%20the,belonging%20to%20a%20certain%20class.\n",
        "    predictions = self.softmax(logits) #normaliser les logits\n",
        "    return x\n",
        "\n",
        "\n",
        "\"\"\"class RCNN (nn.Module):\n",
        "  #Residual convolutional neural network basé sur :\n",
        "    #1. https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "    #2. https://ketanhdoshi.github.io/Audio-ASR/\n",
        "  #\n",
        "  def __init__(self, nbCaracteresPossibles, inChannels, outChannels) :\n",
        "    super.__init__()\n",
        "\n",
        "  def forward(self, input_data) :\n",
        "    pass\n",
        "\"\"\"\n",
        "\n",
        "#algorithme pour le mapping prit sur https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "char_map_str = \"\"\"\n",
        " ' 0\n",
        " <SPACE> 1\n",
        " a 2\n",
        " b 3\n",
        " c 4\n",
        " d 5\n",
        " e 6\n",
        " f 7\n",
        " g 8\n",
        " h 9\n",
        " i 10\n",
        " j 11\n",
        " k 12\n",
        " l 13\n",
        " m 14\n",
        " n 15\n",
        " o 16\n",
        " p 17\n",
        " q 18\n",
        " r 19\n",
        " s 20\n",
        " t 21\n",
        " u 22\n",
        " v 23\n",
        " w 24\n",
        " x 25\n",
        " y 26\n",
        " z 27\n",
        " - 28\n",
        " à 29\n",
        " â 30\n",
        " ä 31\n",
        " é 32\n",
        " è 33\n",
        " ê 34\n",
        " ë 35\n",
        " î 36\n",
        " ï 37\n",
        " ô 38\n",
        " ö 39\n",
        " ù 40\n",
        " û 41\n",
        " ü 42\n",
        " ÿ 43\n",
        " ç 44\n",
        " \"\"\"\n",
        "#associer des caractères à des valeurs numériques\n",
        "class TextTransform:\n",
        "  \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "  def __init__(self, char_map_str):\n",
        "      self.char_map_str = char_map_str\n",
        "      self.char_map = {}\n",
        "      self.index_map = {}\n",
        "      for line in char_map_str.strip().split('\\n'):\n",
        "          ch, index = line.split()\n",
        "          self.char_map[ch] = int(index)\n",
        "          self.index_map[int(index)] = ch\n",
        "      self.index_map[1] = ' '\n",
        "\n",
        "  def text_to_int(self, text):\n",
        "      \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "      int_sequence = []\n",
        "      for c in text:\n",
        "          if c == ' ':\n",
        "              ch = self.char_map['']\n",
        "          else:\n",
        "              ch = self.char_map[c]\n",
        "          int_sequence.append(ch)\n",
        "      return int_sequence\n",
        "\n",
        "  def int_to_text(self, labels):\n",
        "      \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "      string = []\n",
        "      for i in labels:\n",
        "          string.append(self.index_map[i])\n",
        "      return ''.join(string).replace('', ' ')\n",
        "\n",
        "\n",
        "#hyper_parameters = { #paramètres qui décident comment se dérouleral l'entrainement : https://aws.amazon.com/what-is/hyperparameter-tuning/#:~:text=computationally%20intensive%20process.-,What%20are%20hyperparameters%3F,set%20before%20training%20a%20model.\n",
        "#        \"n_lstm_layers\": 2, #nombre de couches de lstm\n",
        "#        \"rnn_dim\": 512, #inChannels\n",
        "#        \"n_caracteres\": 45, #dimensionOutput\n",
        "#        \"dropout\": 0.1, ##réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "#        \"learning_rate\": 0.001, #vitesse d'apprentissage\n",
        "#        \"batch_size\": 20, #nombre d'éléments par endoit qu'on entraine\n",
        "#        \"epochs\": 10 #nombre de fois qu'on entraine le réseau au complet\n",
        "#    }\n",
        "\n",
        "#rnn = BiRNNetwork(hyper_parameters[\"n_caracteres\"], hyper_parameters[\"rnn_dim\"], hyper_parameters[\"n_lstm_layers\"]) #dimensionOutput, inChannels, nombre de couche\n",
        "#summary(rnn.cuda(), (2,1,64,44)) #(nombre de channels, frequence, temps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN81VZbWTGeK"
      },
      "source": [
        "# Entraîner et Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15ed70NTyntC",
        "outputId": "76e8ab9c-3bb2-44a2-9e78-e9a023e06789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utiliation du processeur cuda\n",
            "15 93\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"M\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7afbce77fd87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7afbce73075f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7afbce2ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7afbce2cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7afb00fa558e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7afb00f9d147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5827a508310e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5827a5079a7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5827a5091c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5827a508e087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5827a5079e2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7afbf803d244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5827a5079a7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5827a5072629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5827a5078c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5827a508da64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5827a5079a1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5827a5072629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5827a5071cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x614a (0x5827a5071cfa in /usr/bin/python3)\nframe #24: <unknown function> + 0x1c2afe (0x5827a50ebafe in /usr/bin/python3)\nframe #25: <unknown function> + 0x1c292e (0x5827a50eb92e in /usr/bin/python3)\nframe #26: _PyEval_EvalFrameDefault + 0xbfe (0x5827a506c7ae in /usr/bin/python3)\nframe #27: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #28: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #29: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #31: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #33: <unknown function> + 0x1c2afe (0x5827a50ebafe in /usr/bin/python3)\nframe #34: <unknown function> + 0x28c7b3 (0x5827a51b57b3 in /usr/bin/python3)\nframe #35: _PyEval_EvalFrameDefault + 0xaa0 (0x5827a506c650 in /usr/bin/python3)\nframe #36: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #37: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #38: <unknown function> + 0x13f9c6 (0x5827a50689c6 in /usr/bin/python3)\nframe #39: PyEval_EvalCode + 0x86 (0x5827a515e256 in /usr/bin/python3)\nframe #40: <unknown function> + 0x23ae2d (0x5827a5163e2d in /usr/bin/python3)\nframe #41: <unknown function> + 0x15ac59 (0x5827a5083c59 in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #43: <unknown function> + 0x177ff0 (0x5827a50a0ff0 in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x2568 (0x5827a506e118 in /usr/bin/python3)\nframe #45: <unknown function> + 0x177ff0 (0x5827a50a0ff0 in /usr/bin/python3)\nframe #46: _PyEval_EvalFrameDefault + 0x2568 (0x5827a506e118 in /usr/bin/python3)\nframe #47: <unknown function> + 0x177ff0 (0x5827a50a0ff0 in /usr/bin/python3)\nframe #48: <unknown function> + 0x2557af (0x5827a517e7af in /usr/bin/python3)\nframe #49: <unknown function> + 0x1662ca (0x5827a508f2ca in /usr/bin/python3)\nframe #50: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #51: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #52: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #53: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #54: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #55: <unknown function> + 0x1687f1 (0x5827a50917f1 in /usr/bin/python3)\nframe #56: PyObject_Call + 0x122 (0x5827a5092492 in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x2a27 (0x5827a506e5d7 in /usr/bin/python3)\nframe #58: <unknown function> + 0x1687f1 (0x5827a50917f1 in /usr/bin/python3)\nframe #59: _PyEval_EvalFrameDefault + 0x198c (0x5827a506d53c in /usr/bin/python3)\nframe #60: <unknown function> + 0x200175 (0x5827a5129175 in /usr/bin/python3)\nframe #61: <unknown function> + 0x15ac59 (0x5827a5083c59 in /usr/bin/python3)\nframe #62: <unknown function> + 0x236bc5 (0x5827a515fbc5 in /usr/bin/python3)\nframe #63: <unknown function> + 0x2b2572 (0x5827a51db572 in /usr/bin/python3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-c36e6248cad8>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   \u001b[0mentrainerEpoque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesseur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mtester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesseur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexte_transforme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_caracteres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-c36e6248cad8>\u001b[0m in \u001b[0;36mentrainerEpoque\u001b[0;34m(modele, chargeurDonnees, criterion, optimizer, scheduler, epoch, processeur)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mentrainerEpoque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodele\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchargeurDonnees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesseur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexteAssocie\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchargeurDonnees\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#s'assurer que l'audio et le texte soit au même endroit que le modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesseur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-ebd595b07e10>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtexte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAudioSampleText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#signal informatique et sample rate de notre audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathAudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#mettre le signal au même endroit que sa transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"M\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7afbce77fd87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7afbce73075f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7afbce2ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7afbce2cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7afb00fa558e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7afb00f9d147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5827a508310e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5827a5079a7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5827a5091c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5827a508e087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5827a5079e2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7afbf803d244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5827a5079a7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5827a5072629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5827a5078c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5827a508da64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5827a5079a1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5827a5072629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5827a5071cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x614a (0x5827a5071cfa in /usr/bin/python3)\nframe #24: <unknown function> + 0x1c2afe (0x5827a50ebafe in /usr/bin/python3)\nframe #25: <unknown function> + 0x1c292e (0x5827a50eb92e in /usr/bin/python3)\nframe #26: _PyEval_EvalFrameDefault + 0xbfe (0x5827a506c7ae in /usr/bin/python3)\nframe #27: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #28: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #29: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #31: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #33: <unknown function> + 0x1c2afe (0x5827a50ebafe in /usr/bin/python3)\nframe #34: <unknown function> + 0x28c7b3 (0x5827a51b57b3 in /usr/bin/python3)\nframe #35: _PyEval_EvalFrameDefault + 0xaa0 (0x5827a506c650 in /usr/bin/python3)\nframe #36: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #37: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #38: <unknown function> + 0x13f9c6 (0x5827a50689c6 in /usr/bin/python3)\nframe #39: PyEval_EvalCode + 0x86 (0x5827a515e256 in /usr/bin/python3)\nframe #40: <unknown function> + 0x23ae2d (0x5827a5163e2d in /usr/bin/python3)\nframe #41: <unknown function> + 0x15ac59 (0x5827a5083c59 in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #43: <unknown function> + 0x177ff0 (0x5827a50a0ff0 in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x2568 (0x5827a506e118 in /usr/bin/python3)\nframe #45: <unknown function> + 0x177ff0 (0x5827a50a0ff0 in /usr/bin/python3)\nframe #46: _PyEval_EvalFrameDefault + 0x2568 (0x5827a506e118 in /usr/bin/python3)\nframe #47: <unknown function> + 0x177ff0 (0x5827a50a0ff0 in /usr/bin/python3)\nframe #48: <unknown function> + 0x2557af (0x5827a517e7af in /usr/bin/python3)\nframe #49: <unknown function> + 0x1662ca (0x5827a508f2ca in /usr/bin/python3)\nframe #50: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #51: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #52: _PyEval_EvalFrameDefault + 0x6bd (0x5827a506c26d in /usr/bin/python3)\nframe #53: _PyFunction_Vectorcall + 0x7c (0x5827a50839fc in /usr/bin/python3)\nframe #54: _PyEval_EvalFrameDefault + 0x8ac (0x5827a506c45c in /usr/bin/python3)\nframe #55: <unknown function> + 0x1687f1 (0x5827a50917f1 in /usr/bin/python3)\nframe #56: PyObject_Call + 0x122 (0x5827a5092492 in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x2a27 (0x5827a506e5d7 in /usr/bin/python3)\nframe #58: <unknown function> + 0x1687f1 (0x5827a50917f1 in /usr/bin/python3)\nframe #59: _PyEval_EvalFrameDefault + 0x198c (0x5827a506d53c in /usr/bin/python3)\nframe #60: <unknown function> + 0x200175 (0x5827a5129175 in /usr/bin/python3)\nframe #61: <unknown function> + 0x15ac59 (0x5827a5083c59 in /usr/bin/python3)\nframe #62: <unknown function> + 0x236bc5 (0x5827a515fbc5 in /usr/bin/python3)\nframe #63: <unknown function> + 0x2b2572 (0x5827a51db572 in /usr/bin/python3)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#paramètres essentiels lors de machine learning\n",
        "hyper_parameters = { #paramètres qui décident comment se déroulera l'entrainement : https://aws.amazon.com/what-is/hyperparameter-tuning/#:~:text=computationally%20intensive%20process.-,What%20are%20hyperparameters%3F,set%20before%20training%20a%20model.\n",
        "        \"n_lstm_layers\": 2, #nombre de couches de lstm\n",
        "        \"rnn_dim\": 512, #inChannels\n",
        "        \"n_caracteres\": 45, #dimensionOutput\n",
        "        \"dropout\": 0.1, ##réduit les chances que le réseau de neurone s'adapte à une seule circonstance précise https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "        \"learning_rate\": 0.001, #vitesse d'apprentissage\n",
        "        \"batch_size\": 20, #nombre d'éléments par endoit qu'on entraine\n",
        "        \"epochs\": 10 #nombre de fois qu'on entraine le réseau au complet\n",
        "    }\n",
        "\n",
        "\"\"\"entrainer le RNN à l'aide du CTC Algorithm, un algorithme qui sert à déterminer où sont placer les lettres dans un fichier audio : https://ketanhdoshi.github.io/Audio-ASR/\"\"\"\n",
        "\n",
        "\n",
        "def entrainerEpoque(modele, chargeurDonnees, criterion, optimizer, scheduler, epoch, processeur):\n",
        "  for audio, texteAssocie in chargeurDonnees :\n",
        "    #s'assurer que l'audio et le texte soit au même endroit que le modèle\n",
        "    audio = audio.to(processeur)\n",
        "    texteAssocie = texteAssocie.to(processeur)\n",
        "\n",
        "    #réinitialise les paramètres pour permettre au réseau de neurone de ne pas s'encombrer de paramètres précédents inutiles : https://medium.com/@lazyprogrammerofficial/in-pytorch-why-do-we-need-to-call-optimizer-zero-grad-8e19fdc1ad2f\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #faire une prédiction et déterminer de comment il faut modifier notre réseau de neurone, à l'aide du CTCLoss, pour améliorer la prédiction\n",
        "    prediction = modele(audio)\n",
        "    divergence = criterion(audio, texteAssocie)\n",
        "\n",
        "    #update les valeurs du réseau de neurone avec une backpropagation\n",
        "    divergence.backward() # modifie le poid de chaque paramètres selon la divergence calculée (est-ce qu'il faut augmenter ou diminuer la valeur de ce neurone) : https://en.wikipedia.org/wiki/Backpropagation\n",
        "\n",
        "\n",
        "    #passage au prochain paramètre de l'optimizer et du scheduler\n",
        "    optimizer.step() #descente de gradient : https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step\n",
        "    scheduler.step() #modifie le learning rate : https://discuss.pytorch.org/t/what-does-scheduler-step-do/47764\n",
        "\n",
        "def tester(modele, chargeurDonnees, criterion, epoch, processeur, texte_transforme, caractere_vide) :\n",
        "\n",
        "  modele.eval() # rend fixe certains paramètres qui sont changés durant l'entraînement : https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
        "  divergence_test = 0\n",
        "  erreurLettre = 0\n",
        "  nombreLettre = 0\n",
        "\n",
        "  with torch.no_grad() : #même chose que model.eval() sur d'autres paramètres\n",
        "    for audio, texteAssocie in chargeurDonnees :\n",
        "      #code similaire au train\n",
        "      audio = audio.to(processeur)\n",
        "      texteAssocie = texteAssocie.to(processeur)\n",
        "\n",
        "      prediction = modele(audio)\n",
        "      divergence = criterion(audio, texteAssocie)\n",
        "      divergence_test += divergence\n",
        "\n",
        "      #transformer les valeurs du spectogram en valeurs numériques et enuite en texte\n",
        "      predictions_decode, valeurs_decode = GreedyDecoder(prediction, texteAssocie, texte_transforme, caractere_vide)\n",
        "      nombreLettre += max(len(predictions_decode), len(valeurs_decode))\n",
        "      for i in range(len(predictions_decode)) :\n",
        "        if i < valeurs_decode :\n",
        "          if valeurs_decode != predictions_decode :\n",
        "            erreurLettre += 1\n",
        "\n",
        "      pourcentErreur = erreurLettre/nombreLettre*100\n",
        "      print(f\"Le pourcentage d'erreur à l'époque {epoch} = {pourcentErreur}% pour les lettres et la divergence(loss) = {divergence_test}\")\n",
        "\n",
        "\"\"\"\n",
        "fonction courante dans les speech to text qui compare le résultat attendu avec le résultat obtenu\n",
        "le format de celui-ci est inspiré par : https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "\"\"\"\n",
        "def GreedyDecoder(prediction, texteAssocie, texte_transforme, caractere_vide) :\n",
        "  valeursPrevuesRNN = torch.argmax(prediction) #retourne les valeurs du RNN les plus probables (maximum) #https://pytorch.org/docs/stable/generated/torch.argmax.html\n",
        "  decode = []\n",
        "  attendu = texteAssocie.split(\" \")\n",
        "  for i, valeurs in enumerate(valeursPrevuesRNN) : #valeurs correspond au caracère à chaque endroit possible\n",
        "    for j, index in enumerate(valeurs) : #valeur correspond au caractère à un endroit précis\n",
        "      if index != caractere_vide:\n",
        "        decode.append(index)\n",
        "  decode = texte_transforme.int_to_text(decode)\n",
        "  return decode, valeursPrevuesRNN\n",
        "\n",
        "\n",
        "FICHIER_ANNOTE = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_text.list\"\n",
        "FICHIER_AUDIO = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_wavs.list\"\n",
        "\n",
        "\n",
        "#nombre d'échantillons par secondes dans notre audio\n",
        "SAMPLE_RATE = 22050\n",
        "NOMBRE_ECHANTILLONS = 22050\n",
        "\n",
        "if torch.cuda.is_available(): #détermine ce qui exécute le programme (gpu préférable pour AI audio)\n",
        "    processeur = \"cuda\"\n",
        "else:\n",
        "    processeur = \"cpu\"\n",
        "print(f\"Utiliation du processeur {processeur}\")\n",
        "\n",
        "#le Spectogram de Mel est une échelle logarithmique utilisée pour mieux représenter les différences qu'un humain entend dans un fichier audio ce qui aide à l'analyse sonore\n",
        "melSpectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    SAMPLE_RATE,\n",
        "    n_fft=512, #longueur physique du signal optimale pour la reconnaissance vocale selon : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    hop_length=512, #nombre d'échantillon audio adjacents analysés par la transformée de fourier : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    n_mels=64 #nombre de séparations d'une seule fréquence optimale pour la reconnaisance vocale selon:https://stackoverflow.com/questions/62623975/why-128-mel-bands-are-used-in-mel-spectrograms\n",
        ")\n",
        "\n",
        "#initialisation du Dataset\n",
        "rvd = ReconnaissanceVocaleDataset(FICHIER_ANNOTE, FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE, NOMBRE_ECHANTILLONS, processeur)\n",
        "\n",
        "#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "train_data_loader = DataLoader(rvd,\n",
        "                              hyper_parameters[\"batch_size\"],\n",
        "                              shuffle=True, #https://discuss.pytorch.org/t/how-does-shuffle-in-data-loader-work/49756/7\n",
        "                              )\n",
        "test_data_loader = DataLoader(rvd, #préférablement pas tt le dataset(rvd)\n",
        "                              hyper_parameters[\"batch_size\"],\n",
        "                              shuffle=False,\n",
        "                              )\n",
        "\n",
        "#initialisation du réseau de neurones\n",
        "rnn = BiRNNetwork(hyper_parameters[\"n_caracteres\"], hyper_parameters[\"rnn_dim\"], hyper_parameters[\"n_lstm_layers\"], hyper_parameters[\"dropout\"]).to(processeur)#to(processeur) s'assure que tout s'entraine sur le cuda\n",
        "\n",
        "#le choix du optimizer et du scheduler a été effectué selon l'article : https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "\"\"\"\n",
        "On utilise le CTCLoss function dans le speech to text pour aligner les endroits où il est prédit qu'il y ait des lettres avec les bons neurones (Doshi, 2021)\n",
        "blank permet de ne pas tenir compte des endroits où on prédit qu'il n'y aura pas de caractères : https://distill.pub/2017/ctc/?undefined=&ref=assemblyai.com\n",
        "\"\"\"\n",
        "criterion = nn.CTCLoss(blank = hyper_parameters[\"n_caracteres\"]).to(processeur) #calcul les probabilités selon une fonction prédéfinie : https://nn.readthedocs.io/en/rtd/criterion/index.html\n",
        "\n",
        "optimizer = torch.optim.AdamW(rnn.parameters(), hyper_parameters[\"learning_rate\"]) #change les paramètres du modèle pour améliorer la performance : https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hyper_parameters[\"learning_rate\"],\n",
        "                                                epochs=hyper_parameters[\"epochs\"], # paramètres nécessaires au scheduler :https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\n",
        "                                                steps_per_epoch=len(rvd) # nombre de neurones qu'on entraîne par epoch : https://community.deeplearning.ai/t/request-for-explanation-on-steps-per-epoch-parameter/501777\n",
        "                                                ) #modifie le learning rate pour améliorer la performance : https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90\n",
        "\n",
        "texte_transforme = TextTransform(char_map_str)\n",
        "\n",
        "for epoch in range(hyper_parameters[\"epochs\"]):\n",
        "  entrainerEpoque(rnn, train_data_loader, criterion, optimizer, scheduler, epoch, processeur)\n",
        "  if epoch % 2 == 0 :\n",
        "    tester(rnn, test_data_loader, criterion, epoch, processeur, texte_transforme, hyper_parameters[\"n_caracteres\"])\n",
        "\n",
        "\n",
        "torch.save(rnn.state_dict(), \"feedforwardnet.pth\") #sauvegarder le modele : https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
        "print(\"RNN entraîner sauvegardé sur feedforwardnet.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72q5Kr3CTMyK"
      },
      "source": [
        "# Inférence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzWcdVSdypJD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}