{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex64-1149/VoxNote/blob/IA-VoxNote/VoxNote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptation des données"
      ],
      "metadata": {
        "id": "zeFHx1nkShwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Alphabet dans : https://github.com/dallal9/French-NLP"
      ],
      "metadata": {
        "id": "gO8hYysRS1Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialisation des caractères possibles\n",
        "alphabet = [' ','a', 'à', 'A', 'Á', 'À', 'Â', 'Ä', 'Ã', 'Å', 'r', 'd', 'v', 'k', 's', 'b', 'i', 'e', 'n', 't', 'â', 'm', 'z', 'o', 'é', 'è', 'q', 'u', 'î', '-', 'j', 'g', 'l', 'x', 'y', 'c', 'ê', 'h', 'f', 'p', 'B', 'S', 'û', 'ç', 'ô', 'X', 'ï', 'D', 'N', 'Æ', 'G', 'T', 'C', 'ë', 'K', 'L', 'w', 'ö', 'P', 'I', 'R', \"'\", 'Z', 'E', 'H', 'Ç', 'O', 'M', 'É', 'U', 'V', 'Ð', 'ü', 'È', 'Ê', 'Ë', 'F', 'Q', '3', 'W', 'Í', 'Ì', 'Î', 'Ï', 'J', 'Y', '/', 'Ñ', 'Ó', 'Ò', 'Ô', 'Ö', 'Õ', 'Ø', 'ù', 'þ', 'Ú', 'Ù', 'Û', 'Ü', 'Ý']\n",
        "chiffres = ['1','2','3','4','5','6','7','8','9','0']\n",
        "caracteresPossible = alphabet + chiffres\n"
      ],
      "metadata": {
        "id": "jKqKD9Si5JK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#adaptation de https://github.com/musikalkemist/pytorchforaudio\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class ReconnaissanceVocaleDataset(Dataset) :\n",
        "\n",
        "  #Initialise les aurguments de la classe dataset:\n",
        "  # 1.fichier annotation(fichier txt qui contient tous les noms des fichiers audio (wav))\n",
        "  # 2.fichier audio ( contient tous les fichiers audio(wav))\n",
        "  def __init__(self,FICHIER_ANNOTE,FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE):\n",
        "    self.fichierAnnotations=pd.read_csv(FICHIER_ANNOTE)\n",
        "    self.fichierAudio=FICHIER_AUDIO\n",
        "    self.melSpectrogram = melSpectrogram\n",
        "    self.SAMPLE_RATE_VOULU = SAMPLE_RATE\n",
        "\n",
        "  #retourne le nombre de fichiers dans notre dataset\n",
        "  def __len__(self):\n",
        "    return len(self.fichierAnnotations)\n",
        "\n",
        "  #retourne l'audio ainsi que son fichier texte associé\n",
        "  def __getitem__(self, index):\n",
        "    pathAudio = self.getAudioSamplePath(index)\n",
        "    texte = self.getAudioSampleText(index)\n",
        "    #signal informatique et sample rate de notre audio\n",
        "    signal, sr = torchaudio.load(pathAudio)\n",
        "    #normaliser l'audio et le mettre dans le spectogram de mel\n",
        "    signal = self.resampleSiNecessaire(signal, sr)\n",
        "    signal = self.combinerSiNecessaire(signal)\n",
        "    signal = self.melSpectrogram(signal)\n",
        "    return signal, texte\n",
        "\n",
        "  #mettre tous les fichiers audios au même nombre d'échantillons par seconde\n",
        "  def resampleSiNecessaire(self, signal, sr):\n",
        "    if sr != self.SAMPLE_RATE_VOULU :\n",
        "      resampler = torchaudio.transforms.Resample(sr, self.SAMPLE_RATE_VOULU)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  #s'assurer que l'audio ne contient qu'une entrée et sortie (que le son ne soit pas stéréo) pour le normaliser\n",
        "  def combinerSiNecessaire(self, signal):\n",
        "    if signal.shape[0] > 1 :\n",
        "      signal = torch.mean(signal, dim=0, keepdim= True)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  #retourne le chemin pour avoir le bon fichier audio à un certain index du FICHIER_ANNOTE\n",
        "  def getAudioSamplePath(self, index):\n",
        "    return self.fichierAudio[index]\n",
        "\n",
        "  #retourne le fichier texte associé à l'audio d'un certain index du FICHIER_AUDIO\n",
        "  def getAudioSampleText(self, index):\n",
        "    return self.fichierAnnotations[index]\n",
        "\n",
        "\n",
        "FICHIER_ANNOTE = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_text.list\"\n",
        "FICHIER_AUDIO = \"/content/drive/MyDrive/Colab Notebooks/SiwisFrenchSpeechSynthesisDatabase/lists/all_wavs.list\"\n",
        "\n",
        "\n",
        "#nombre d'échantillons par secondes dans notre audio\n",
        "SAMPLE_RATE = 22050\n",
        "#le Spectogram de Mel est une échelle logarithmique utilisée pour mieux représenter les différences qu'un humain entend dans un fichier audio ce qui aide à l'analyse sonore\n",
        "melSpectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    SAMPLE_RATE,\n",
        "    n_fft=512, #longueur physique du signal optimale pour la reconnaissance vocale selon : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    hop_length=512, #nombre d'échantillon audio adjacents analysés par la transformée de fourier : https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "    n_mels=64 #nombre de séparations d'une seule fréquence optimale pour la reconnaisance vocale selon:https://stackoverflow.com/questions/62623975/why-128-mel-bands-are-used-in-mel-spectrograms\n",
        ")\n",
        "\n",
        "\n",
        "rvd = ReconnaissanceVocaleDataset(FICHIER_ANNOTE, FICHIER_AUDIO, melSpectrogram, SAMPLE_RATE)\n"
      ],
      "metadata": {
        "id": "a4QSC_m-8aL-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utilisé pour extraire le zip de google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#!unzip file.zip"
      ],
      "metadata": {
        "id": "iSiu0ku4h3lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Réseau de neurone"
      ],
      "metadata": {
        "id": "ydEmnsTDS43i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraîner"
      ],
      "metadata": {
        "id": "QN81VZbWTGeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tester\n"
      ],
      "metadata": {
        "id": "-derVaPjTKNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inférence"
      ],
      "metadata": {
        "id": "72q5Kr3CTMyK"
      }
    }
  ]
}